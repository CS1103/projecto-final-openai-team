[![Review Assignment Due Date](https://classroom.github.com/assets/deadline-readme-button-22041afd0340ce965d47ae6ef1cefeee28c7c493a6346c4f15d667ab976d596c.svg)](https://classroom.github.com/a/Lj3YlzJp)
# Proyecto Final 2025-1: AI Neural Network
## **CS2013 Programaci√≥n III** ¬∑ Informe Final

## **Descripci√≥n**

> Ejemplo: Implementaci√≥n de una red neuronal multicapa en C++ para clasificaci√≥n de d√≠gitos manuscritos.

## Contenidos

1. [Datos generales](#datos-generales)
2. [Requisitos e instalaci√≥n](#requisitos-e-instalaci√≥n)
3. [Investigaci√≥n te√≥rica](#1-investigaci√≥n-te√≥rica)
4. [Dise√±o e implementaci√≥n](#2-dise√±o-e-implementaci√≥n)
5. [Ejecuci√≥n](#3-ejecuci√≥n)
6. [An√°lisis del rendimiento](#4-an√°lisis-del-rendimiento)
7. [Trabajo en equipo](#5-trabajo-en-equipo)
8. [Conclusiones](#6-conclusiones)
9. [Bibliograf√≠a](#7-bibliograf√≠a)
10. [Licencia](#licencia)
---

## Datos generales

* **Tema**: Redes Neuronales en AI
* **Grupo**: `group_3_custom_name`
* **Integrantes**:

  * Alumno A ‚Äì 209900001 (Responsable de investigaci√≥n te√≥rica)
  * Alumno B ‚Äì 209900002 (Desarrollo de la arquitectura)
  * Alumno C ‚Äì 209900003 (Implementaci√≥n del modelo)
  * Alumno D ‚Äì 209900004 (Pruebas y benchmarking)
  * Alumno E ‚Äì 209900005 (Documentaci√≥n y demo)

> *Nota: Reemplazar nombres y roles reales.*

---

## Requisitos e instalaci√≥n

1. **Compilador**: GCC 11 o superior
2. **Dependencias**:

   * CMake 3.18+
   * Eigen 3.4
   * \[Otra librer√≠a opcional]
3. **Instalaci√≥n**:

   ```bash
   git clone https://github.com/CS1103/projecto-final-openai-team.git
   cd proyecto-final-openai-team
   g++ -o programa main.cpp
   ./programa
   ```

> *Ejemplo de repositorio y comandos, ajustar seg√∫n proyecto.*

---

## 1. Investigaci√≥n te√≥rica  

### üéØ Objetivo

Explorar fundamentos y arquitecturas de redes neuronales, comprendiendo su evoluci√≥n, funcionamiento interno y algoritmos de entrenamiento.

---

### üß† 1.1 Historia y evoluci√≥n de las redes neuronales

Las redes neuronales artificiales (ANNs) nacieron como una idea inspirada en la estructura biol√≥gica del cerebro humano.  
El primer modelo matem√°tico fue propuesto por **McCulloch y Pitts** en 1943, quienes demostraron que una red simple de neuronas artificiales pod√≠a representar funciones l√≥gicas b√°sicas [3].

En 1958, el psic√≥logo **Frank Rosenblatt** desarroll√≥ el *perceptr√≥n*, un algoritmo capaz de clasificar entradas linealmente separables.  
Aunque fue un avance innovador para su tiempo, presentaba limitaciones: no pod√≠a resolver problemas como la funci√≥n XOR.  
Estas carencias llevaron a una disminuci√≥n del inter√©s en los a√±os 70.

El renacimiento lleg√≥ en los a√±os 80 con la introducci√≥n del algoritmo de **retropropagaci√≥n del error (backpropagation)**, que permiti√≥ entrenar redes con m√∫ltiples capas ocultas (*MLP*) [5].  
Este avance t√©cnico marc√≥ un antes y un despu√©s, haciendo posible modelar relaciones no lineales de forma m√°s eficiente.

A partir de la d√©cada de 2010, el panorama cambi√≥ radicalmente:

- El crecimiento del *big data*.  
- La aparici√≥n de hardware especializado (como las GPUs).  
- Y la necesidad de resolver tareas m√°s complejas...

...hicieron que las redes neuronales profundas (*deep learning*) tomaran el protagonismo en aplicaciones del mundo real [1].

> üöÄ Hoy en d√≠a, las ANNs son clave en tecnolog√≠as como **ChatGPT**, sistemas de reconocimiento facial, veh√≠culos aut√≥nomos y diagn√≥sticos m√©dicos asistidos por IA.

---

### üèóÔ∏è 1.2 Principales arquitecturas: MLP, CNN y RNN

Las arquitecturas de redes neuronales definen c√≥mo se organizan las neuronas artificiales y qu√© tipo de datos pueden procesar eficientemente.

---

#### üîπ MLP (Perceptr√≥n Multicapa)

El MLP es la base de muchas redes modernas.  
Est√° compuesto por varias capas de neuronas (una capa de entrada, una o m√°s capas ocultas y una capa de salida), donde cada neurona est√° conectada a todas las de la capa siguiente [2].  
Es ideal para tareas de clasificaci√≥n, regresi√≥n y reconocimiento de patrones cuando los datos no tienen estructura espacial ni secuencial.

> üß† A pesar de su simplicidad, los MLP pueden aproximar funciones complejas si se entrenan correctamente y con suficientes capas.

---

#### üîπ CNN (Convolutional Neural Networks)

Las CNNs est√°n especialmente dise√±adas para trabajar con datos estructurados en forma de matrices, como im√°genes.  
Utilizan capas convolucionales que aplican filtros para detectar caracter√≠sticas locales (bordes, texturas, colores, formas), lo que reduce el n√∫mero de par√°metros necesarios y mejora el rendimiento [1].

**Aplicaciones destacadas de las CNNs:**
- üì∑ Reconocimiento facial en tiempo real.  
- ü©∫ Clasificaci√≥n de im√°genes m√©dicas (tumores, fracturas, etc.).  
- üöó Sistemas de visi√≥n en veh√≠culos aut√≥nomos.

---

#### üîπ RNN (Recurrent Neural Networks)

Las RNNs est√°n dise√±adas para procesar secuencias de datos.  
A diferencia de las redes tradicionales, poseen conexiones recurrentes que les permiten ‚Äúrecordar‚Äù informaci√≥n previa [2].

**Usos comunes de las RNNs:**
- üìù Procesamiento de lenguaje natural.  
- üåê Traducci√≥n autom√°tica.  
- üí¨ An√°lisis de sentimientos.  
- üìà Predicci√≥n de series temporales (finanzas, clima, etc.).

> üß¨ Existen versiones mejoradas como **LSTM** y **GRU**, que solucionan problemas como el desvanecimiento del gradiente y permiten memorizar secuencias m√°s largas.

---

### ‚öôÔ∏è 1.3 Algoritmos de entrenamiento: backpropagation y optimizadores

El entrenamiento de una red neuronal consiste en ajustar sus par√°metros (pesos y sesgos) para minimizar la diferencia entre la salida esperada y la obtenida.  
Para lograr esto, se utilizan dos elementos clave:

---

#### üîÑ Backpropagation

Es un algoritmo que aplica la **regla de la cadena** del c√°lculo diferencial para distribuir el error de salida hacia las capas anteriores.  
Cada peso se actualiza en funci√≥n del gradiente de la funci√≥n de p√©rdida respecto a ese peso, permitiendo que la red aprenda patrones complejos [5].

> üí° Su impacto fue tal que permiti√≥ pasar de redes simples a redes profundas con m√∫ltiples capas ocultas.

---

#### ‚öôÔ∏è Optimizadores

El **optimizador** es el encargado de decidir c√≥mo actualizar los pesos de la red durante el proceso de aprendizaje.  
Algunos de los m√°s conocidos y utilizados son:

- **SGD (Stochastic Gradient Descent):**  
  Actualiza los pesos usando solo una muestra o mini-lote. Es eficiente pero puede ser sensible al *learning rate*.

- **Adam (Adaptive Moment Estimation):**  
  Combina ideas de *momentum* y adaptaci√≥n din√°mica. Es robusto, eficiente y muy popular en la pr√°ctica [4].

- **RMSprop y Adagrad:**  
  Ideales para datos dispersos o ruidosos. Ajustan la tasa de aprendizaje de manera adaptativa seg√∫n la frecuencia de actualizaci√≥n de cada par√°metro.

> ‚úÖ Una buena elecci√≥n del optimizador y de la tasa de aprendizaje (*learning rate*) puede marcar la diferencia entre una red que converge eficientemente y otra que nunca llega a aprender correctamente.

---

## 2. Dise√±o e implementaci√≥n

### 2.1 Arquitectura de la soluci√≥n

* **Patrones de dise√±o**: ejemplo: Factory para capas, Strategy para optimizadores.
* **Estructura de carpetas (ejemplo)**:

  ```
  proyecto-final/
  ‚îú‚îÄ‚îÄ src/
  ‚îÇ   ‚îú‚îÄ‚îÄ layers/
  ‚îÇ   ‚îú‚îÄ‚îÄ optimizers/
  ‚îÇ   ‚îî‚îÄ‚îÄ main.cpp
  ‚îú‚îÄ‚îÄ tests/
  ‚îî‚îÄ‚îÄ docs/
  ```

### 2.2 Manual de uso y casos de prueba

* **C√≥mo ejecutar**: `./build/neural_net_demo input.csv output.csv`
* **Casos de prueba**:

  * Test unitario de capa densa.
  * Test de funci√≥n de activaci√≥n ReLU.
  * Test de convergencia en dataset de ejemplo.

> *Personalizar rutas, comandos y casos reales.*

---

## 3. Ejecuci√≥n

> **Demo de ejemplo**: Video/demo alojado en `docs/demo.mp4`.
> Pasos:
>
> 1. Preparar datos de entrenamiento (formato CSV).
> 2. Ejecutar comando de entrenamiento.
> 3. Evaluar resultados con script de validaci√≥n.

---

## 4. An√°lisis del rendimiento

* **M√©tricas de ejemplo**:

  * Iteraciones: 1000 √©pocas.
  * Tiempo total de entrenamiento: 2m30s.
  * Precisi√≥n final: 92.5%.
* **Ventajas/Desventajas**:

  * * C√≥digo ligero y dependencias m√≠nimas.
  * ‚Äì Sin paralelizaci√≥n, rendimiento limitado.
* **Mejoras futuras**:

  * Uso de BLAS para multiplicaciones (Justificaci√≥n).
  * Paralelizar entrenamiento por lotes (Justificaci√≥n).

---

## 5. Trabajo en equipo

| Tarea                     | Miembro  | Rol                       |
| ------------------------- | -------- | ------------------------- |
| Investigaci√≥n te√≥rica     | Alumno A | Documentar bases te√≥ricas |
| Dise√±o de la arquitectura | Alumno B | UML y esquemas de clases  |
| Implementaci√≥n del modelo | Alumno C | C√≥digo C++ de la NN       |
| Pruebas y benchmarking    | Alumno D | Generaci√≥n de m√©tricas    |
| Documentaci√≥n y demo      | Alumno E | Tutorial y video demo     |

> *Actualizar con tareas y nombres reales.*

---

## 6. Conclusiones

* **Logros**: Implementar NN desde cero, validar en dataset de ejemplo.
* **Evaluaci√≥n**: Calidad y rendimiento adecuados para prop√≥sito acad√©mico.
* **Aprendizajes**: Profundizaci√≥n en backpropagation y optimizaci√≥n.
* **Recomendaciones**: Escalar a datasets m√°s grandes y optimizar memoria.

---

##  7. Bibliograf√≠a 

[1] Y. LeCun, Y. Bengio, and G. Hinton, ‚ÄúDeep learning,‚Äù *Nature*, vol. 521, no. 7553, pp. 436‚Äì444, 2015.  
[2] I. Goodfellow, Y. Bengio, and A. Courville, *Deep Learning*, MIT Press, 2016.  
[3] S. Haykin, *Neural Networks and Learning Machines*, 3rd ed., Pearson, 2009.  
[4] C. M. Bishop, *Pattern Recognition and Machine Learning*, Springer, 2006.  
[5] D. E. Rumelhart, G. E. Hinton, and R. J. Williams, ‚ÄúLearning representations by back-propagating errors,‚Äù *Nature*, vol. 323, pp. 533‚Äì536, 1986.

---

### Licencia

Este proyecto usa la licencia **MIT**. Ver [LICENSE](LICENSE) para detalles.

---
